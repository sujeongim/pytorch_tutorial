{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "practice 1 복습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y prediction before training : y_pred = 0.0\n",
      "epoch 0 : loss = 30.0, w = 0.30000001192092896, b=0.10000000149011612\n",
      "epoch 50 : loss = 0.04600245878100395, w = 1.821869134902954, b=0.5231990218162537\n",
      "epoch 100 : loss = 0.03408462926745415, w = 1.8468074798583984, b=0.4504048526287079\n",
      "epoch 150 : loss = 0.025254584848880768, w = 1.8681354522705078, b=0.3876981735229492\n",
      "epoch 200 : loss = 0.01871202513575554, w = 1.8864939212799072, b=0.3337215781211853\n",
      "epoch 250 : loss = 0.013864448294043541, w = 1.9022966623306274, b=0.2872597575187683\n",
      "epoch 300 : loss = 0.010272667743265629, w = 1.9158991575241089, b=0.24726659059524536\n",
      "epoch 350 : loss = 0.0076114218682050705, w = 1.9276080131530762, b=0.21284149587154388\n",
      "epoch 400 : loss = 0.005639577750116587, w = 1.9376866817474365, b=0.18320895731449127\n",
      "epoch 450 : loss = 0.004178564064204693, w = 1.9463618993759155, b=0.1577020287513733\n",
      "y prediction after training : y_pred = 9.904609605669975\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([1, 2, 3, 4], dtype=np.float32)\n",
    "Y = np.array([2, 4, 6, 8], dtype=np.float32)\n",
    "\n",
    "w = np.array(0.0, dtype=np.float32)\n",
    "b = np.array(0.0, dtype=np.float32)\n",
    "\n",
    "# forward\n",
    "def forward(x):\n",
    "    return w * x + b\n",
    "\n",
    "# loss\n",
    "# MSE loss\n",
    "def loss(y_pred, y):\n",
    "    return ((y_pred-y)**2).mean()\n",
    "\n",
    "# backward\n",
    "# MSE loss = 1/N * (y_pred-y)**2 = 1/N * (w * x + b - y)**2\n",
    "# dl/dw = 1/N * 2 * (w * x + b - y) * x\n",
    "# dl/db = 1/N * 2 * (w * x + b - y) * 1\n",
    "def gradient_w(x, y):\n",
    "    return (2 * (w * x + b - y) * x).mean()\n",
    "\n",
    "def gradient_b(x, y):\n",
    "    return (2 * (w * x + b - y)).mean()\n",
    "\n",
    "\n",
    "print(f'y prediction before training : y_pred = {forward(5)}')\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 500\n",
    "for epoch in range(n_iters):\n",
    "    # forward\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(y_pred, Y)\n",
    "\n",
    "    # gradient\n",
    "    grad_w = gradient_w(X, Y)\n",
    "    grad_b = gradient_b(X, Y)\n",
    "\n",
    "    # update w, b\n",
    "    w -= learning_rate * grad_w\n",
    "    b -= learning_rate * grad_b\n",
    "\n",
    "    if epoch % 50 == 0 :\n",
    "        print(f'epoch {epoch} : loss = {l}, w = {w}, b={b}')\n",
    "    \n",
    "\n",
    "print(f'y prediction after training : y_pred = {forward(5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "practice 2 복습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y prediction before training : y_pred = 0.0\n",
      "epoch 0 : loss = 30.0, w = 0.29999998211860657, b=0.09999999403953552\n",
      "epoch 50 : loss = 0.04600245878100395, w = 1.821869134902954, b=0.5231990218162537\n",
      "epoch 100 : loss = 0.03408462926745415, w = 1.8468074798583984, b=0.4504048526287079\n",
      "epoch 150 : loss = 0.025254584848880768, w = 1.8681354522705078, b=0.3876981735229492\n",
      "epoch 200 : loss = 0.01871202513575554, w = 1.8864939212799072, b=0.3337215781211853\n",
      "epoch 250 : loss = 0.013864448294043541, w = 1.9022966623306274, b=0.2872597575187683\n",
      "epoch 300 : loss = 0.010272667743265629, w = 1.9158991575241089, b=0.24726659059524536\n",
      "epoch 350 : loss = 0.0076114218682050705, w = 1.9276080131530762, b=0.21284149587154388\n",
      "epoch 400 : loss = 0.005639577750116587, w = 1.9376866817474365, b=0.18320895731449127\n",
      "epoch 450 : loss = 0.004178564064204693, w = 1.9463618993759155, b=0.1577020287513733\n",
      "y prediction after training : y_pred = 9.904609680175781\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "b = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# forward\n",
    "def forward(x):\n",
    "    return w * x + b\n",
    "\n",
    "# loss\n",
    "# MSE loss\n",
    "def loss(y_pred, y):\n",
    "    return ((y_pred-y)**2).mean()\n",
    "\n",
    "print(f'y prediction before training : y_pred = {forward(5)}')\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 500\n",
    "for epoch in range(n_iters):\n",
    "    # forward\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(y_pred, Y)\n",
    "\n",
    "    # gradient\n",
    "    l.backward()\n",
    "    grad_w = w.grad\n",
    "    grad_b = b.grad\n",
    "\n",
    "    # update w, b\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * grad_w\n",
    "        b -= learning_rate * grad_b\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "\n",
    "    if epoch % 50 == 0 :\n",
    "        print(f'epoch {epoch} : loss = {l}, w = {w}, b={b}')\n",
    "    \n",
    "\n",
    "print(f'y prediction after training : y_pred = {forward(5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## practice 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< general training pipeline >\n",
    "1) Design model (input_size, output_size, forward pass)\n",
    "2) Construct Loss and Optimizer\n",
    "3) Training loop\n",
    "   - forward pass : compute prediction\n",
    "   - backward pass : gradients\n",
    "   - update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y prediction before training : y_pred = 0.0\n",
      "epoch 0 : loss = 30.0, w = 0.29999998211860657, b=0.09999999403953552\n",
      "epoch 50 : loss = 0.04600245878100395, w = 1.821869134902954, b=0.5231990218162537\n",
      "epoch 100 : loss = 0.03408462926745415, w = 1.8468074798583984, b=0.4504048526287079\n",
      "epoch 150 : loss = 0.025254584848880768, w = 1.8681354522705078, b=0.3876981735229492\n",
      "epoch 200 : loss = 0.01871202513575554, w = 1.8864939212799072, b=0.3337215781211853\n",
      "epoch 250 : loss = 0.013864448294043541, w = 1.9022966623306274, b=0.2872597575187683\n",
      "epoch 300 : loss = 0.010272667743265629, w = 1.9158991575241089, b=0.24726659059524536\n",
      "epoch 350 : loss = 0.0076114218682050705, w = 1.9276080131530762, b=0.21284149587154388\n",
      "epoch 400 : loss = 0.005639577750116587, w = 1.9376866817474365, b=0.18320895731449127\n",
      "epoch 450 : loss = 0.004178564064204693, w = 1.9463618993759155, b=0.1577020287513733\n",
      "y prediction after training : y_pred = 9.904609680175781\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "b = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "\n",
    "def forward(x):\n",
    "    return w * x + b\n",
    "\n",
    "print(f'y prediction before training : y_pred = {forward(5)}')\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 500\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD([w, b], lr=learning_rate)\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(y_pred, Y)\n",
    "\n",
    "    # gradients = backward pass\n",
    "    l.backward() # dl/dw\n",
    "\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 50 == 0 :\n",
    "        print(f'epoch {epoch} : loss = {l}, w = {w}, b={b}')\n",
    "    \n",
    "\n",
    "print(f'y prediction after training : y_pred = {forward(5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n",
      "Prediction before training : f(5) = -1.423\n",
      "<generator object Module.parameters at 0x7fb22aa70f20>\n",
      "epoch 1 : loss = 36.40452575683594, w = -0.080, b= 0.733\n",
      "epoch 51 : loss = 0.20759570598602295, w = 1.622, b= 1.111\n",
      "epoch 101 : loss = 0.15381528437137604, w = 1.675, b= 0.957\n",
      "epoch 151 : loss = 0.11396743357181549, w = 1.720, b= 0.824\n",
      "epoch 201 : loss = 0.08444271981716156, w = 1.759, b= 0.709\n",
      "epoch 251 : loss = 0.06256675720214844, w = 1.792, b= 0.610\n",
      "epoch 301 : loss = 0.04635809361934662, w = 1.821, b= 0.525\n",
      "epoch 351 : loss = 0.034348420798778534, w = 1.846, b= 0.452\n",
      "epoch 401 : loss = 0.025450000539422035, w = 1.868, b= 0.389\n",
      "epoch 451 : loss = 0.01885686255991459, w = 1.886, b= 0.335\n",
      "y prediction after training : y_pred = 9.797\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# f = w * x + b\n",
    "\n",
    "# f = 2 * x\n",
    "# (n_samples, n_features) 형태로 shape 바꿔야 함\n",
    "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32) # (n_samples, n_features)\n",
    "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32) # (n_samples, y_n_features)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "n_samples, y_n_features = Y.shape\n",
    "print(n_samples, n_features)\n",
    "\n",
    "# test tensor\n",
    "X_test = torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "# model 정의\n",
    "# pytorch model이 파라미터가 뭔지도 알기 때문에 w, b도 정의할 필요없음\n",
    "# 파라미터를 자동으로 어떻게 알지...? -> 어떤 모델(레이어) 쓰느야에 따라 다르게 파라미터가 정해져 있는 듯 \n",
    "input_size = n_features\n",
    "output_size = y_n_features\n",
    "model = nn.Linear(n_features, y_n_features) # (input_size, output_size)\n",
    "\n",
    "print(f'Prediction before training : f(5) = {model(X_test).item():.3f}')\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 500\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "# [w, b] -> model.parameters\n",
    "print(model.parameters())\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    y_pred = model(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "\n",
    "    # gradients -> backward pass\n",
    "    l.backward()\n",
    "\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero_grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 50 == 0 :\n",
    "        [w, b] = model.parameters()\n",
    "        print(f'epoch {epoch+1} : loss = {l}, w = {w[0][0].item():.3f}, b= {b[0].item():.3f}') # {w[0][0].item():.3f}\n",
    "    \n",
    "\n",
    "print(f'y prediction after training : y_pred = {model(X_test).item():.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n",
      "Prediction before training : f(5) = 9.797\n",
      "<generator object Module.parameters at 0x7fb1fce884a0>\n",
      "epoch 1 : loss = 0.013971743173897266, w = 1.902, b= 0.288\n",
      "epoch 51 : loss = 0.010352179408073425, w = 1.916, b= 0.248\n",
      "epoch 101 : loss = 0.007670307531952858, w = 1.927, b= 0.214\n",
      "epoch 151 : loss = 0.005683228373527527, w = 1.937, b= 0.184\n",
      "epoch 201 : loss = 0.004210909828543663, w = 1.946, b= 0.158\n",
      "epoch 251 : loss = 0.00312002282589674, w = 1.954, b= 0.136\n",
      "epoch 301 : loss = 0.0023117540404200554, w = 1.960, b= 0.117\n",
      "epoch 351 : loss = 0.0017128606559708714, w = 1.966, b= 0.101\n",
      "epoch 401 : loss = 0.0012691137380897999, w = 1.970, b= 0.087\n",
      "epoch 451 : loss = 0.000940340687520802, w = 1.975, b= 0.075\n",
      "y prediction after training : y_pred = 9.955\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# f = w * x + b\n",
    "\n",
    "# f = 2 * x\n",
    "# (n_samples, n_features) 형태로 shape 바꿔야 함\n",
    "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32) # (n_samples, n_features)\n",
    "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32) # (n_samples, y_n_features)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "n_samples, y_n_features = Y.shape\n",
    "print(n_samples, n_features)\n",
    "\n",
    "# test tensor\n",
    "X_test = torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "# custom model 정의\n",
    "# pytorch model이 파라미터가 뭔지도 알기 때문에 w, b도 정의할 필요없음\n",
    "# 파라미터를 자동으로 어떻게 알지...? -> 어떤 모델(레이어) 쓰느야에 따라 다르게 파라미터가 정해져 있는 듯 \n",
    "input_size = n_features\n",
    "output_size = y_n_features\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        # define layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "\n",
    "print(f'Prediction before training : f(5) = {model(X_test).item():.3f}')\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 500\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "# [w, b] -> model.parameters\n",
    "print(model.parameters())\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    y_pred = model(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "\n",
    "    # gradients -> backward pass\n",
    "    l.backward()\n",
    "\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero_grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 50 == 0 :\n",
    "        [w, b] = model.parameters()\n",
    "        print(f'epoch {epoch+1} : loss = {l}, w = {w[0][0].item():.3f}, b= {b[0].item():.3f}') # {w[0][0].item():.3f}\n",
    "    \n",
    "\n",
    "print(f'y prediction after training : y_pred = {model(X_test).item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2e22769a2f415e715c8de30e0c105102333e31bf626ae6886f4b46712f0b5c6e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
