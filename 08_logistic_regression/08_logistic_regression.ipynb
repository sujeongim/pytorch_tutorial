{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0) prepare date\n",
    "\n",
    "1) model 정의 (input, output)\n",
    "2) loss & optimizer\n",
    "3) Training Loop\n",
    "   - forward pass\n",
    "   - loss\n",
    "   - backward pass\n",
    "   - update\n",
    "   - zero grad\n",
    "   - print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569 30\n",
      "epoch 10 : loss = 0.5600, [Parameter containing:\n",
      "tensor([[-0.0637,  0.0759,  0.0632, -0.1740, -0.1214, -0.1903,  0.0229, -0.1072,\n",
      "          0.1093, -0.1329, -0.1329,  0.0647, -0.0206,  0.1274, -0.0673,  0.0606,\n",
      "          0.0364,  0.1155,  0.1783,  0.0982,  0.0439, -0.1398, -0.2099,  0.0245,\n",
      "          0.0578,  0.0679,  0.0950, -0.0945, -0.1111,  0.0323]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.0032], requires_grad=True)]\n",
      "epoch 20 : loss = 0.4575, [Parameter containing:\n",
      "tensor([[-0.0863,  0.0607,  0.0401, -0.1959, -0.1329, -0.2111, -0.0014, -0.1325,\n",
      "          0.0966, -0.1346, -0.1521,  0.0622, -0.0397,  0.1089, -0.0680,  0.0470,\n",
      "          0.0247,  0.0990,  0.1752,  0.0911,  0.0198, -0.1556, -0.2344,  0.0017,\n",
      "          0.0438,  0.0474,  0.0718, -0.1207, -0.1259,  0.0194]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([0.0097], requires_grad=True)]\n",
      "epoch 30 : loss = 0.3934, [Parameter containing:\n",
      "tensor([[-0.1044,  0.0479,  0.0215, -0.2136, -0.1415, -0.2269, -0.0204, -0.1526,\n",
      "          0.0867, -0.1349, -0.1674,  0.0601, -0.0547,  0.0942, -0.0684,  0.0370,\n",
      "          0.0160,  0.0864,  0.1730,  0.0862,  0.0003, -0.1689, -0.2542, -0.0167,\n",
      "          0.0326,  0.0314,  0.0535, -0.1416, -0.1379,  0.0096]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([0.0218], requires_grad=True)]\n",
      "epoch 40 : loss = 0.3495, [Parameter containing:\n",
      "tensor([[-0.1197,  0.0369,  0.0059, -0.2285, -0.1483, -0.2394, -0.0360, -0.1692,\n",
      "          0.0787, -0.1344, -0.1802,  0.0584, -0.0671,  0.0820, -0.0687,  0.0295,\n",
      "          0.0094,  0.0764,  0.1713,  0.0829, -0.0162, -0.1804, -0.2707, -0.0322,\n",
      "          0.0233,  0.0185,  0.0384, -0.1588, -0.1480,  0.0019]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([0.0330], requires_grad=True)]\n",
      "epoch 50 : loss = 0.3174, [Parameter containing:\n",
      "tensor([[-0.1329,  0.0272, -0.0076, -0.2414, -0.1537, -0.2496, -0.0492, -0.1833,\n",
      "          0.0721, -0.1332, -0.1912,  0.0570, -0.0777,  0.0714, -0.0690,  0.0237,\n",
      "          0.0043,  0.0684,  0.1699,  0.0807, -0.0305, -0.1907, -0.2851, -0.0456,\n",
      "          0.0154,  0.0077,  0.0258, -0.1735, -0.1567, -0.0044]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([0.0434], requires_grad=True)]\n",
      "epoch 60 : loss = 0.2929, [Parameter containing:\n",
      "tensor([[-0.1447,  0.0183, -0.0195, -0.2528, -0.1583, -0.2581, -0.0605, -0.1957,\n",
      "          0.0665, -0.1317, -0.2009,  0.0558, -0.0871,  0.0621, -0.0691,  0.0192,\n",
      "          0.0004,  0.0617,  0.1690,  0.0793, -0.0432, -0.1999, -0.2979, -0.0577,\n",
      "          0.0084, -0.0014,  0.0148, -0.1864, -0.1644, -0.0096]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([0.0532], requires_grad=True)]\n",
      "epoch 70 : loss = 0.2734, [Parameter containing:\n",
      "tensor([[-0.1553,  0.0103, -0.0302, -0.2632, -0.1621, -0.2654, -0.0705, -0.2066,\n",
      "          0.0617, -0.1299, -0.2096,  0.0548, -0.0954,  0.0536, -0.0692,  0.0156,\n",
      "         -0.0027,  0.0562,  0.1682,  0.0786, -0.0547, -0.2084, -0.3093, -0.0685,\n",
      "          0.0021, -0.0093,  0.0052, -0.1978, -0.1713, -0.0140]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([0.0623], requires_grad=True)]\n",
      "epoch 80 : loss = 0.2576, [Parameter containing:\n",
      "tensor([[-0.1649,  0.0028, -0.0399, -0.2726, -0.1654, -0.2717, -0.0794, -0.2165,\n",
      "          0.0574, -0.1278, -0.2176,  0.0539, -0.1029,  0.0459, -0.0693,  0.0128,\n",
      "         -0.0052,  0.0515,  0.1676,  0.0783, -0.0652, -0.2163, -0.3198, -0.0784,\n",
      "         -0.0036, -0.0163, -0.0033, -0.2080, -0.1775, -0.0177]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([0.0710], requires_grad=True)]\n",
      "epoch 90 : loss = 0.2443, [Parameter containing:\n",
      "tensor([[-0.1738, -0.0042, -0.0489, -0.2813, -0.1683, -0.2772, -0.0875, -0.2255,\n",
      "          0.0537, -0.1256, -0.2249,  0.0531, -0.1098,  0.0388, -0.0694,  0.0106,\n",
      "         -0.0070,  0.0474,  0.1672,  0.0785, -0.0749, -0.2237, -0.3294, -0.0876,\n",
      "         -0.0088, -0.0225, -0.0110, -0.2173, -0.1833, -0.0210]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([0.0792], requires_grad=True)]\n",
      "epoch 100 : loss = 0.2330, [Parameter containing:\n",
      "tensor([[-0.1820, -0.0107, -0.0572, -0.2894, -0.1709, -0.2820, -0.0948, -0.2337,\n",
      "          0.0503, -0.1233, -0.2317,  0.0524, -0.1162,  0.0322, -0.0694,  0.0089,\n",
      "         -0.0085,  0.0439,  0.1669,  0.0789, -0.0839, -0.2307, -0.3383, -0.0961,\n",
      "         -0.0137, -0.0280, -0.0179, -0.2258, -0.1886, -0.0238]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([0.0869], requires_grad=True)]\n",
      "accuracy = 0.9211\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "'''0) prepare data'''\n",
    "bc = datasets.load_breast_cancer()\n",
    "X, y = bc.data, bc.target\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "print(n_samples, n_features)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234) # what is random state?\n",
    "\n",
    "# scale\n",
    "sc = StandardScaler() # mean = 0 & unit variance # 왜 linear regression 에서는 안하고 logistic 에서는 하나? -> activation function 때문에 그런 것 같다. \n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.fit_transform(X_test)\n",
    "\n",
    "# numpy to torch\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "\n",
    "# y reshape\n",
    "y_train = y_train.view(y_train.shape[0], 1)\n",
    "y_test = y_test.view(y_test.shape[0], 1)\n",
    "\n",
    "\n",
    "'''1) model - input, output'''\n",
    "# f = wx+b, sigmois at the end\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_predicted = torch.sigmoid(self.linear(x))\n",
    "        return y_predicted\n",
    "model = LogisticRegression(n_features, 1)\n",
    "\n",
    "'''2) loss & optimizer'''\n",
    "learning_rate = 0.01\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "'''3) Training loop'''\n",
    "num_epoch = 100\n",
    "for epoch in range(num_epoch):\n",
    "    # forward and loss\n",
    "    y_pred = model(X_train)\n",
    "    loss = criterion(y_pred,y_train)\n",
    "\n",
    "    # backward\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # print\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"epoch {epoch+1} : loss = {loss.item():.4f}, {[params for params in model.parameters()]}\") \n",
    "\n",
    "# accuracy\n",
    "with torch.no_grad():\n",
    "    y_predicted = model(X_test)\n",
    "    y_predicted_cls = y_predicted.round()\n",
    "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'accuracy = {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2e22769a2f415e715c8de30e0c105102333e31bf626ae6886f4b46712f0b5c6e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
